#!/usr/bin/env python3
"""
Ollama AI Recommendation Service - Enhanced Streaming & Verbose Mode
èåˆç‰ˆæœ¬ï¼šä¸²æµç”Ÿæˆ + è©³ç´°æ—¥èªŒ + å½©è‰²è¼¸å‡º
æ”¯æŒæœ¬åœ° Ollama å’Œäº‘ç«¯ Ollama API (ä½¿ç”¨å®˜æ–¹ Ollama Python Client)
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import time
import re
import os
import requests
from datetime import datetime
from dotenv import load_dotenv
from ollama import Client

# Load .env file
load_dotenv()

app = Flask(__name__)
CORS(app)

# Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "")
MODEL = os.getenv("MODEL", "llama3.2:latest")
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "2"))
RETRY_DELAY = int(os.getenv("RETRY_DELAY", "1"))


def print_timestamp():
    """è¿”å›æ ¼å¼åŒ–çš„æ™‚é–“æˆ³"""
    return datetime.now().strftime("%H:%M:%S.%f")[:-3]


def print_log(emoji, message, color=""):
    """Print colored log with timestamp"""
    colors = {
        "blue": "\033[94m",
        "green": "\033[92m",
        "yellow": "\033[93m",
        "red": "\033[91m",
        "cyan": "\033[96m",
        "magenta": "\033[95m",
        "reset": "\033[0m"
    }
    c = colors.get(color, "")
    reset = colors["reset"] if color else ""
    print(f"{c}[{print_timestamp()}] {emoji} {message}{reset}", flush=True)


class OllamaStreamClient:
    """Ollama client using official Ollama Python library"""

    def __init__(self, url=OLLAMA_URL, model=MODEL, api_key=OLLAMA_API_KEY):
        self.url = url
        self.model = model
        self.api_key = api_key

        # Initialize Ollama client
        # If API key is provided, pass it via headers for cloud authentication
        client_kwargs = {"host": url}
        if api_key:
            # Pass API key as Bearer token in Authorization header
            client_kwargs["headers"] = {
                "Authorization": f"Bearer {api_key}"
            }
            print_log("ğŸ”‘", f"ä½¿ç”¨ API Key è¿æ¥é›²ç«¯ Ollama: {url}", "cyan")
        else:
            print_log("ğŸ ", f"è¿æ¥æœ¬åœ° Ollama: {url}", "cyan")

        self.client = Client(**client_kwargs)

    def _clean_json(self, text):
        """Remove markdown code blocks from response"""
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        return text.strip()

    def _validate_recommendations(self, data):
        """Validate recommendation data structure"""
        if not isinstance(data, list):
            return False
        for item in data:
            required_keys = ['book_id', 'reason', 'score']
            if not all(k in item for k in required_keys):
                return False
        return True

    def generate_stream(self, prompt, temp=0.7, retries=MAX_RETRIES):
        """Generate recommendations with streaming output using Ollama"""
        last_error = None

        for attempt in range(retries):
            try:
                print_log(
                    "ğŸ¯",
                    f"Ollama å‘¼å« (å˜—è©¦ {attempt + 1}/{retries})",
                    "cyan"
                )
                print_log("ğŸ“", f"æ¨¡å‹: {self.model}", "blue")
                print_log("ğŸŒ¡ï¸", f"Temperature: {temp}", "blue")

                # Show prompt preview
                preview_len = 200
                prompt_preview = (
                    prompt[:preview_len] + "..."
                    if len(prompt) > preview_len
                    else prompt
                )
                print_log("ğŸ’¬", "Prompt é è¦½:", "magenta")
                print(f"    {prompt_preview}")
                print()

                print_log("ğŸš€", "é–‹å§‹ Ollama ä¸²æµç”Ÿæˆ...", "yellow")
                print_log("ğŸ“¡", "=" * 60, "yellow")

                # Prepare messages in OpenAI-compatible format
                messages = [
                    {
                        'role': 'user',
                        'content': prompt,
                    }
                ]

                # Stream response from Ollama using official client
                full_content = ""
                chunk_count = 0

                # Green color for AI output
                print("\n\033[92m", end="", flush=True)

                for part in self.client.chat(
                    model=self.model,
                    messages=messages,
                    stream=True,
                    options={'temperature': temp}
                ):
                    if 'message' in part and 'content' in part['message']:
                        content = part['message']['content']
                        print(content, end="", flush=True)
                        full_content += content
                        chunk_count += 1

                # Reset color
                print("\033[0m\n", flush=True)
                print_log("ğŸ“¡", "=" * 60, "yellow")
                print_log(
                    "âœ…",
                    f"Ollama ç”Ÿæˆå®Œæˆï¼å…± {chunk_count} å€‹ç‰‡æ®µ",
                    "green"
                )
                print_log("ğŸ“Š", f"ç¸½é•·åº¦: {len(full_content)} å­—å…ƒ", "blue")
                print()

                # Clean and parse JSON
                print_log("ğŸ”§", "æ¸…ç†å’Œè§£æ JSON...", "blue")
                content = self._clean_json(full_content)

                try:
                    result = json.loads(content)
                    print_log("âœ…", "JSON è§£ææˆåŠŸ", "green")

                    if self._validate_recommendations(result):
                        rec_count = len(result)
                        print_log(
                            "âœ…",
                            f"é©—è­‰æˆåŠŸï¼Œæ‰¾åˆ° {rec_count} ç­†æ¨è–¦",
                            "green"
                        )
                        return result
                    else:
                        print_log("âŒ", "æ¨è–¦æ ¼å¼é©—è­‰å¤±æ•—", "red")
                        raise Exception("Invalid recommendation format")

                except json.JSONDecodeError as e:
                    print_log("âŒ", f"JSON è§£æå¤±æ•—: {e}", "red")
                    print_log("ğŸ“„", "åŸå§‹å›æ‡‰å…§å®¹:", "yellow")
                    print(f"    {content[:500]}")
                    raise

            except Exception as e:
                last_error = str(e)
                print_log("âŒ", f"éŒ¯èª¤: {last_error}", "red")

                if attempt < retries - 1:
                    print_log(
                        "â³",
                        f"ç­‰å¾… {RETRY_DELAY} ç§’å¾Œé‡è©¦...",
                        "yellow"
                    )
                    time.sleep(RETRY_DELAY)
                else:
                    print_log("ğŸ’¥", "æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—", "red")

        raise Exception(f"All retries failed: {last_error}")


def fallback_recommendations(books, count=3):
    """Fallback when AI fails - simple but works"""
    print_log("ğŸ”„", "ä½¿ç”¨ Fallback æ¨è–¦æ©Ÿåˆ¶", "yellow")
    return [
        {
            'book_id': book['id'],
            'reason': f"æ¨è–¦é–±è®€ã€Š{book['title']}ã€‹by {book['author']}",
            'score': 0.7
        }
        for book in books[:count]
    ]


ollama = OllamaStreamClient()


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    print_log("ğŸ’š", "å¥åº·æª¢æŸ¥è«‹æ±‚", "green")
    try:
        # Try to list models to verify connection
        models = ollama.client.list()
        return jsonify({
            "status": "healthy",
            "model": MODEL,
            "ollama_url": OLLAMA_URL,
            "using_api_key": bool(OLLAMA_API_KEY),
            "available_models": len(models.get('models', []))
        }), 200
    except Exception as e:
        print_log("âŒ", f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}", "red")
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 503


@app.route('/generate-personal-recommendations', methods=['POST'])
def generate_personal():
    """Generate personal recommendations based on user history"""
    print_log("ğŸ¯", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°å€‹äººåŒ–æ¨è–¦è«‹æ±‚", "cyan")

    data = request.json
    user_profile = data.get('user_profile', {})
    books = data.get('available_books', [])

    history_len = len(user_profile.get('borrow_history', []))
    print_log("ğŸ‘¤", f"ä½¿ç”¨è€…å€Ÿé–±æ­·å²: {history_len} æœ¬æ›¸", "blue")
    print_log("ğŸ“š", f"å¯å€Ÿé–±æ›¸ç±: {len(books)} æœ¬", "blue")
    print()

    # Build prompt
    history = user_profile.get('borrow_history', [])
    history_titles = [item['title'] for item in history]
    book_list = [
        f"{b['id']}: {b['title']} by {b['author']}"
        for b in books
    ]

    history_str = ', '.join(history_titles) if history_titles else 'No history'
    prompt = f"""Based on reading history: {history_str}

Recommend 3 books from this list:
{chr(10).join(book_list)}

Return ONLY a JSON array like this (no markdown, no explanation):
[
  {{"book_id": "001", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.85}},
  {{"book_id": "002", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.80}},
  {{"book_id": "003", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.75}}
]"""

    try:
        recommendations = ollama.generate_stream(prompt, temp=0.7)
        print_log(
            "ğŸ‰",
            f"æˆåŠŸç”Ÿæˆ {len(recommendations)} ç­†æ¨è–¦",
            "green"
        )
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": recommendations,
            "source": "ai"
        })

    except Exception as e:
        print_log("âš ï¸", f"AI ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ Fallback: {e}", "yellow")
        fallback = fallback_recommendations(books, 3)
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": fallback,
            "source": "fallback"
        })


@app.route('/generate-related-recommendations', methods=['POST'])
def generate_related():
    """Generate related book recommendations"""
    print_log("ğŸ¯", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°ç›¸é—œæ¨è–¦è«‹æ±‚", "cyan")

    data = request.json
    current_book = data.get('current_book', {})
    related_books = data.get('related_books', [])

    print_log("ğŸ“–", f"ç•¶å‰æ›¸ç±: {current_book.get('title')}", "blue")
    print_log("ğŸ“š", f"ç›¸é—œæ›¸ç±: {len(related_books)} æœ¬", "blue")
    print()

    book_list = [
        f"{b['id']}: {b['title']} by {b['author']}"
        for b in related_books
    ]

    current_title = current_book.get('title')
    current_author = current_book.get('author')
    prompt = f"""For readers who liked: {current_title} by {current_author}

Recommend 3 related books from:
{chr(10).join(book_list)}

Return ONLY a JSON array (no markdown):
[
  {{"book_id": "002", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.8}},
  {{"book_id": "003", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.75}},
  {{"book_id": "004", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.70}}
]"""

    try:
        recommendations = ollama.generate_stream(prompt, temp=0.6)
        rec_count = len(recommendations)
        print_log("ğŸ‰", f"æˆåŠŸç”Ÿæˆ {rec_count} ç­†ç›¸é—œæ¨è–¦", "green")
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": recommendations,
            "source": "ai"
        })

    except Exception as e:
        print_log("âš ï¸", f"AI ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ Fallback: {e}", "yellow")
        fallback = fallback_recommendations(related_books, 3)
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": fallback,
            "source": "fallback"
        })


def get_default_system_prompt():
    """
    ç²å–é è¨­çš„ system promptï¼ˆä¸ä½¿ç”¨ RAGï¼‰
    """
    return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åœ–æ›¸é¤¨ AI åŠ©ç†ï¼Œè² è²¬å”åŠ©ä½¿ç”¨è€…è™•ç†åœ–æ›¸é¤¨ç›¸é—œå•é¡Œã€‚

ä½ çš„è·è²¬ï¼š
1. å›ç­”åœ–æ›¸é¤¨è¦å‰‡å’Œå¸¸è¦‹å•é¡Œ
2. å”åŠ©ä½¿ç”¨è€…æŸ¥è©¢æ›¸ç±è³‡è¨Š
3. æä¾›å€Ÿé–±æ­·å²æŸ¥è©¢
4. èªªæ˜å€Ÿé‚„æ›¸æµç¨‹
5. æä¾›åœ–æ›¸é¤¨çµ±è¨ˆè³‡è¨Š

å›ç­”åŸå‰‡ï¼š
- å›ç­”è¦ç°¡æ½”ã€æº–ç¢ºã€å‹å–„
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
- å¦‚æœä¸ç¢ºå®šç­”æ¡ˆï¼Œèª å¯¦å‘ŠçŸ¥ä½¿ç”¨è€…
- æ¶‰åŠå…·é«”æ“ä½œæ™‚ï¼Œæä¾›æ¸…æ¥šçš„æ­¥é©Ÿèªªæ˜"""


@app.route('/chat', methods=['POST'])
def chat():
    """
    èŠå¤©ç«¯é»ï¼ˆæ”¯æ´ RAGï¼‰

    Request Body:
        {
            "message": "ä½¿ç”¨è€…è¨Šæ¯",
            "history": [{"role": "user", "content": "..."}, ...],
            "context": "{...}"  // å¯é¸ï¼šChatContext JSON
        }
    """
    print_log("ğŸ’¬", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°èŠå¤©è«‹æ±‚", "cyan")

    try:
        data = request.json
        user_message = data.get('message', '')
        history = data.get('history', [])
        context_json = data.get('context', None)  # æ–°å¢ï¼šå¯é¸çš„ context

        if not user_message:
            return jsonify({'success': False, 'message': 'è¨Šæ¯ä¸èƒ½ç‚ºç©º'}), 400

        print_log("ğŸ‘¤", f"ä½¿ç”¨è€…è¨Šæ¯: {user_message[:50]}...", "blue")
        print_log("ğŸ“", f"å°è©±æ­·å²: {len(history)} è¼ª", "blue")

        # æ§‹å»ºè¨Šæ¯åˆ—è¡¨
        messages = []

        # 1. æ±ºå®š system promptï¼ˆæ ¹æ“šæ˜¯å¦æœ‰ contextï¼‰
        if context_json:
            # æœ‰ contextï¼šä½¿ç”¨ RAG system prompt
            try:
                from rag_prompt_builder import build_rag_system_prompt, validate_context

                # è§£æ context
                context_data = json.loads(context_json)

                # é©—è­‰ context
                is_valid, error_msg = validate_context(context_data)
                if not is_valid:
                    print_log("âš ï¸", f"Context validation failed: {error_msg}", "yellow")
                    # é©—è­‰å¤±æ•—ï¼Œä½¿ç”¨é è¨­ prompt
                    system_prompt = get_default_system_prompt()
                else:
                    # æ§‹å»º RAG system prompt
                    system_prompt = build_rag_system_prompt(context_data)
                    has_data = context_data.get('hasData', False)
                    print_log("âœ…", f"Using RAG system prompt (hasData={has_data})", "green")

            except Exception as e:
                print_log("âŒ", f"Error building RAG prompt: {e}", "red")
                import traceback
                traceback.print_exc()
                # ç™¼ç”ŸéŒ¯èª¤ï¼Œä½¿ç”¨é è¨­ prompt
                system_prompt = get_default_system_prompt()
        else:
            # æ²’æœ‰ contextï¼šä½¿ç”¨é è¨­ system prompt
            system_prompt = get_default_system_prompt()
            print_log("â„¹ï¸", "Using default system prompt (no context provided)", "blue")

        # 2. æ·»åŠ  system prompt
        messages.append({
            "role": "system",
            "content": system_prompt
        })

        # 3. æ·»åŠ æ­·å²è¨˜éŒ„ï¼ˆé™åˆ¶åœ¨æœ€è¿‘ 5 è¼ªå°è©±ï¼‰
        if history:
            recent_history = history[-10:]  # 5 è¼ª = 10 æ¢è¨Šæ¯ï¼ˆuser + assistantï¼‰
            for msg in recent_history:
                if msg.get('role') in ['user', 'assistant']:
                    messages.append({
                        "role": msg['role'],
                        "content": msg['content']
                    })

        # 4. æ·»åŠ ç•¶å‰ä½¿ç”¨è€…è¨Šæ¯
        messages.append({
            "role": "user",
            "content": user_message
        })

        # 5. å‘¼å« Ollama API
        print_log("ğŸ“¤", f"Sending to Ollama: {len(messages)} messages", "yellow")

        response = ollama.client.chat(
            model=MODEL,
            messages=messages,
            options={'temperature': 0.7}
        )

        assistant_message = response['message']['content']
        print_log("âœ…", f"Ollama response received: {len(assistant_message)} chars", "green")
        print_log("ğŸ’¬", "=" * 70, "cyan")
        print()

        return jsonify({
            'success': True,
            'message': assistant_message
        })

    except requests.exceptions.Timeout:
        print_log("â±ï¸", "Ollama request timeout", "red")
        print_log("ğŸ’¬", "=" * 70, "cyan")
        print()
        return jsonify({
            'success': False,
            'message': 'AI æœå‹™éŸ¿æ‡‰è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦'
        }), 504

    except Exception as e:
        print_log("âŒ", f"Chat error: {str(e)}", "red")
        import traceback
        traceback.print_exc()
        print_log("ğŸ’¬", "=" * 70, "cyan")
        print()
        return jsonify({
            'success': False,
            'message': f'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
        }), 500


if __name__ == '__main__':
    print()
    print("=" * 70)
    print("  ğŸ¤– Ollama AI Recommendation Service - Official Client")
    print("=" * 70)
    print()
    print("  ğŸ“ Service URL: http://localhost:8888")
    print("  ğŸ”— Ollama URL:", OLLAMA_URL)
    api_status = "âœ… Configured" if OLLAMA_API_KEY else "âŒ Not set"
    print("  ğŸ”‘ API Key:", api_status)
    print("  ğŸ§  Model:", MODEL)
    print("  ğŸ”„ Max Retries:", MAX_RETRIES)
    print("  â±ï¸  Retry Delay:", RETRY_DELAY, "seconds")
    print()
    print("  ğŸ¯ Features:")
    print("     - âœ¨ Real-time Ollama streaming (official client)")
    print("     - ğŸ¨ Colored logs with timestamps")
    print("     - ğŸ“Š Detailed progress tracking")
    print("     - ğŸ” Detailed error messages")
    print("     - ğŸ”„ Retry mechanism with fallback")
    print("     - ğŸ’š Health check endpoint")
    print("     - ğŸ”‘ Support for cloud Ollama API")
    print()
    print("  ğŸ“Š Endpoints:")
    print("     - GET  /health")
    print("     - POST /generate-personal-recommendations")
    print("     - POST /generate-related-recommendations")
    print("     - POST /chat (AI Chatbot)")
    print()
    print("=" * 70)
    print()

    # Enable debug mode for hot reload during development
    app.run(host='0.0.0.0', port=8888, debug=True)
