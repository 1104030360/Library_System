#!/usr/bin/env python3
"""
Ollama AI Recommendation Service - Enhanced Streaming & Verbose Mode
èåˆç‰ˆæœ¬ï¼šä¸²æµç”Ÿæˆ + è©³ç´°æ—¥èªŒ + å½©è‰²è¼¸å‡º
æ”¯æŒæœ¬åœ° Ollama å’Œäº‘ç«¯ Ollama API (ä½¿ç”¨å®˜æ–¹ Ollama Python Client)
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import time
import re
import os
from datetime import datetime
from dotenv import load_dotenv
from ollama import Client

# Load .env file
load_dotenv()

app = Flask(__name__)
CORS(app)

# Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "")
MODEL = os.getenv("MODEL", "llama3.2:latest")
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "2"))
RETRY_DELAY = int(os.getenv("RETRY_DELAY", "1"))


def print_timestamp():
    """è¿”å›æ ¼å¼åŒ–çš„æ™‚é–“æˆ³"""
    return datetime.now().strftime("%H:%M:%S.%f")[:-3]


def print_log(emoji, message, color=""):
    """Print colored log with timestamp"""
    colors = {
        "blue": "\033[94m",
        "green": "\033[92m",
        "yellow": "\033[93m",
        "red": "\033[91m",
        "cyan": "\033[96m",
        "magenta": "\033[95m",
        "reset": "\033[0m"
    }
    c = colors.get(color, "")
    reset = colors["reset"] if color else ""
    print(f"{c}[{print_timestamp()}] {emoji} {message}{reset}", flush=True)


class OllamaStreamClient:
    """Ollama client using official Ollama Python library"""

    def __init__(self, url=OLLAMA_URL, model=MODEL, api_key=OLLAMA_API_KEY):
        self.url = url
        self.model = model
        self.api_key = api_key

        # Initialize Ollama client
        # If API key is provided, pass it via headers for cloud authentication
        client_kwargs = {"host": url}
        if api_key:
            # Pass API key as Bearer token in Authorization header
            client_kwargs["headers"] = {
                "Authorization": f"Bearer {api_key}"
            }
            print_log("ğŸ”‘", f"ä½¿ç”¨ API Key è¿æ¥é›²ç«¯ Ollama: {url}", "cyan")
        else:
            print_log("ğŸ ", f"è¿æ¥æœ¬åœ° Ollama: {url}", "cyan")

        self.client = Client(**client_kwargs)

    def _clean_json(self, text):
        """Remove markdown code blocks from response"""
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        return text.strip()

    def _validate_recommendations(self, data):
        """Validate recommendation data structure"""
        if not isinstance(data, list):
            return False
        for item in data:
            required_keys = ['book_id', 'reason', 'score']
            if not all(k in item for k in required_keys):
                return False
        return True

    def generate_stream(self, prompt, temp=0.7, retries=MAX_RETRIES):
        """Generate recommendations with streaming output using Ollama"""
        last_error = None

        for attempt in range(retries):
            try:
                print_log(
                    "ğŸ¯",
                    f"Ollama å‘¼å« (å˜—è©¦ {attempt + 1}/{retries})",
                    "cyan"
                )
                print_log("ğŸ“", f"æ¨¡å‹: {self.model}", "blue")
                print_log("ğŸŒ¡ï¸", f"Temperature: {temp}", "blue")

                # Show prompt preview
                preview_len = 200
                prompt_preview = (
                    prompt[:preview_len] + "..."
                    if len(prompt) > preview_len
                    else prompt
                )
                print_log("ğŸ’¬", "Prompt é è¦½:", "magenta")
                print(f"    {prompt_preview}")
                print()

                print_log("ğŸš€", "é–‹å§‹ Ollama ä¸²æµç”Ÿæˆ...", "yellow")
                print_log("ğŸ“¡", "=" * 60, "yellow")

                # Prepare messages in OpenAI-compatible format
                messages = [
                    {
                        'role': 'user',
                        'content': prompt,
                    }
                ]

                # Stream response from Ollama using official client
                full_content = ""
                chunk_count = 0

                # Green color for AI output
                print("\n\033[92m", end="", flush=True)

                for part in self.client.chat(
                    model=self.model,
                    messages=messages,
                    stream=True,
                    options={'temperature': temp}
                ):
                    if 'message' in part and 'content' in part['message']:
                        content = part['message']['content']
                        print(content, end="", flush=True)
                        full_content += content
                        chunk_count += 1

                # Reset color
                print("\033[0m\n", flush=True)
                print_log("ğŸ“¡", "=" * 60, "yellow")
                print_log(
                    "âœ…",
                    f"Ollama ç”Ÿæˆå®Œæˆï¼å…± {chunk_count} å€‹ç‰‡æ®µ",
                    "green"
                )
                print_log("ğŸ“Š", f"ç¸½é•·åº¦: {len(full_content)} å­—å…ƒ", "blue")
                print()

                # Clean and parse JSON
                print_log("ğŸ”§", "æ¸…ç†å’Œè§£æ JSON...", "blue")
                content = self._clean_json(full_content)

                try:
                    result = json.loads(content)
                    print_log("âœ…", "JSON è§£ææˆåŠŸ", "green")

                    if self._validate_recommendations(result):
                        rec_count = len(result)
                        print_log(
                            "âœ…",
                            f"é©—è­‰æˆåŠŸï¼Œæ‰¾åˆ° {rec_count} ç­†æ¨è–¦",
                            "green"
                        )
                        return result
                    else:
                        print_log("âŒ", "æ¨è–¦æ ¼å¼é©—è­‰å¤±æ•—", "red")
                        raise Exception("Invalid recommendation format")

                except json.JSONDecodeError as e:
                    print_log("âŒ", f"JSON è§£æå¤±æ•—: {e}", "red")
                    print_log("ğŸ“„", "åŸå§‹å›æ‡‰å…§å®¹:", "yellow")
                    print(f"    {content[:500]}")
                    raise

            except Exception as e:
                last_error = str(e)
                print_log("âŒ", f"éŒ¯èª¤: {last_error}", "red")

                if attempt < retries - 1:
                    print_log(
                        "â³",
                        f"ç­‰å¾… {RETRY_DELAY} ç§’å¾Œé‡è©¦...",
                        "yellow"
                    )
                    time.sleep(RETRY_DELAY)
                else:
                    print_log("ğŸ’¥", "æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—", "red")

        raise Exception(f"All retries failed: {last_error}")


def fallback_recommendations(books, count=3):
    """Fallback when AI fails - simple but works"""
    print_log("ğŸ”„", "ä½¿ç”¨ Fallback æ¨è–¦æ©Ÿåˆ¶", "yellow")
    return [
        {
            'book_id': book['id'],
            'reason': f"æ¨è–¦é–±è®€ã€Š{book['title']}ã€‹by {book['author']}",
            'score': 0.7
        }
        for book in books[:count]
    ]


ollama = OllamaStreamClient()


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    print_log("ğŸ’š", "å¥åº·æª¢æŸ¥è«‹æ±‚", "green")
    try:
        # Try to list models to verify connection
        models = ollama.client.list()
        return jsonify({
            "status": "healthy",
            "model": MODEL,
            "ollama_url": OLLAMA_URL,
            "using_api_key": bool(OLLAMA_API_KEY),
            "available_models": len(models.get('models', []))
        }), 200
    except Exception as e:
        print_log("âŒ", f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}", "red")
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 503


@app.route('/generate-personal-recommendations', methods=['POST'])
def generate_personal():
    """Generate personal recommendations based on user history"""
    print_log("ğŸ¯", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°å€‹äººåŒ–æ¨è–¦è«‹æ±‚", "cyan")

    data = request.json
    user_profile = data.get('user_profile', {})
    books = data.get('available_books', [])

    history_len = len(user_profile.get('borrow_history', []))
    print_log("ğŸ‘¤", f"ä½¿ç”¨è€…å€Ÿé–±æ­·å²: {history_len} æœ¬æ›¸", "blue")
    print_log("ğŸ“š", f"å¯å€Ÿé–±æ›¸ç±: {len(books)} æœ¬", "blue")
    print()

    # Build prompt
    history = user_profile.get('borrow_history', [])
    history_titles = [item['title'] for item in history]
    book_list = [
        f"{b['id']}: {b['title']} by {b['author']}"
        for b in books
    ]

    history_str = ', '.join(history_titles) if history_titles else 'No history'
    prompt = f"""Based on reading history: {history_str}

Recommend 3 books from this list:
{chr(10).join(book_list)}

Return ONLY a JSON array like this (no markdown, no explanation):
[
  {{"book_id": "001", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.85}},
  {{"book_id": "002", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.80}},
  {{"book_id": "003", "reason": "æ¨è–¦ç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.75}}
]"""

    try:
        recommendations = ollama.generate_stream(prompt, temp=0.7)
        print_log(
            "ğŸ‰",
            f"æˆåŠŸç”Ÿæˆ {len(recommendations)} ç­†æ¨è–¦",
            "green"
        )
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": recommendations,
            "source": "ai"
        })

    except Exception as e:
        print_log("âš ï¸", f"AI ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ Fallback: {e}", "yellow")
        fallback = fallback_recommendations(books, 3)
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": fallback,
            "source": "fallback"
        })


@app.route('/generate-related-recommendations', methods=['POST'])
def generate_related():
    """Generate related book recommendations"""
    print_log("ğŸ¯", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°ç›¸é—œæ¨è–¦è«‹æ±‚", "cyan")

    data = request.json
    current_book = data.get('current_book', {})
    related_books = data.get('related_books', [])

    print_log("ğŸ“–", f"ç•¶å‰æ›¸ç±: {current_book.get('title')}", "blue")
    print_log("ğŸ“š", f"ç›¸é—œæ›¸ç±: {len(related_books)} æœ¬", "blue")
    print()

    book_list = [
        f"{b['id']}: {b['title']} by {b['author']}"
        for b in related_books
    ]

    current_title = current_book.get('title')
    current_author = current_book.get('author')
    prompt = f"""For readers who liked: {current_title} by {current_author}

Recommend 3 related books from:
{chr(10).join(book_list)}

Return ONLY a JSON array (no markdown):
[
  {{"book_id": "002", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.8}},
  {{"book_id": "003", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.75}},
  {{"book_id": "004", "reason": "ç›¸é—œç†ç”±ï¼ˆä¸­æ–‡ï¼‰", "score": 0.70}}
]"""

    try:
        recommendations = ollama.generate_stream(prompt, temp=0.6)
        rec_count = len(recommendations)
        print_log("ğŸ‰", f"æˆåŠŸç”Ÿæˆ {rec_count} ç­†ç›¸é—œæ¨è–¦", "green")
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": recommendations,
            "source": "ai"
        })

    except Exception as e:
        print_log("âš ï¸", f"AI ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ Fallback: {e}", "yellow")
        fallback = fallback_recommendations(related_books, 3)
        print_log("ğŸ¯", "=" * 70, "cyan")
        print()
        return jsonify({
            "success": True,
            "recommendations": fallback,
            "source": "fallback"
        })


@app.route('/chat', methods=['POST'])
def chat():
    """Simple chat endpoint - Linus style: just works, no fancy stuff"""
    print_log("ğŸ’¬", "=" * 70, "cyan")
    print_log("ğŸ“¨", "æ”¶åˆ°èŠå¤©è«‹æ±‚", "cyan")

    data = request.json
    user_message = data.get('message', '')
    history = data.get('history', [])

    print_log("ğŸ‘¤", f"ä½¿ç”¨è€…è¨Šæ¯: {user_message[:50]}...", "blue")
    print_log("ğŸ“", f"å°è©±æ­·å²: {len(history)} è¼ª", "blue")
    print()

    # Build messages array - simple and straightforward
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸­å¤§åœ–æ›¸é¤¨çš„ AI åŠ©æ‰‹ã€‚å”åŠ©ä½¿ç”¨è€…è§£ç­”é—œæ–¼åœ–æ›¸é¤¨å€Ÿé‚„ç³»çµ±çš„å•é¡Œã€‚å›ç­”è¦ç°¡æ½”ã€å‹å–„ã€ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
        }
    ]

    # Add conversation history (keep last 5 rounds only - Linus: simple limits)
    for msg in history[-10:]:  # Last 5 rounds = 10 messages (user + assistant)
        messages.append({
            "role": msg.get('role', 'user'),
            "content": msg.get('content', '')
        })

    # Add current user message
    messages.append({
        "role": "user",
        "content": user_message
    })

    try:
        print_log("ğŸ¤–", f"å‘¼å« Ollama ({MODEL})...", "yellow")

        # Call Ollama - no streaming for simplicity (Linus: start simple)
        response = ollama.client.chat(
            model=MODEL,
            messages=messages,
            options={'temperature': 0.7}
        )

        assistant_message = response['message']['content']
        print_log("âœ…", f"å›æ‡‰ç”ŸæˆæˆåŠŸ ({len(assistant_message)} å­—å…ƒ)", "green")
        print_log("ğŸ’¬", "=" * 70, "cyan")
        print()

        return jsonify({
            "success": True,
            "message": assistant_message
        })

    except Exception as e:
        print_log("âŒ", f"éŒ¯èª¤: {str(e)}", "red")
        print_log("ğŸ’¬", "=" * 70, "cyan")
        print()

        # Fallback response - always have a backup (Linus: never fail silently)
        return jsonify({
            "success": False,
            "message": "æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ã€‚è«‹ç¨å¾Œå†è©¦æˆ–è¯çµ¡åœ–æ›¸é¤¨ç®¡ç†å“¡ã€‚"
        }), 503


if __name__ == '__main__':
    print()
    print("=" * 70)
    print("  ğŸ¤– Ollama AI Recommendation Service - Official Client")
    print("=" * 70)
    print()
    print("  ğŸ“ Service URL: http://localhost:8888")
    print("  ğŸ”— Ollama URL:", OLLAMA_URL)
    api_status = "âœ… Configured" if OLLAMA_API_KEY else "âŒ Not set"
    print("  ğŸ”‘ API Key:", api_status)
    print("  ğŸ§  Model:", MODEL)
    print("  ğŸ”„ Max Retries:", MAX_RETRIES)
    print("  â±ï¸  Retry Delay:", RETRY_DELAY, "seconds")
    print()
    print("  ğŸ¯ Features:")
    print("     - âœ¨ Real-time Ollama streaming (official client)")
    print("     - ğŸ¨ Colored logs with timestamps")
    print("     - ğŸ“Š Detailed progress tracking")
    print("     - ğŸ” Detailed error messages")
    print("     - ğŸ”„ Retry mechanism with fallback")
    print("     - ğŸ’š Health check endpoint")
    print("     - ğŸ”‘ Support for cloud Ollama API")
    print()
    print("  ğŸ“Š Endpoints:")
    print("     - GET  /health")
    print("     - POST /generate-personal-recommendations")
    print("     - POST /generate-related-recommendations")
    print("     - POST /chat (AI Chatbot)")
    print()
    print("=" * 70)
    print()

    # Enable debug mode for hot reload during development
    app.run(host='0.0.0.0', port=8888, debug=True)
