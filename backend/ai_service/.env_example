# Ollama Configuration
# 请复制此文件为 .env 并填写实际值

# Ollama API URL
# 本地: http://localhost:11434
# Docker 容器内: http://ollama-base:11434
# 云端 Ollama: 使用云端服务提供的 URL
OLLAMA_URL=http://localhost:11434

# Ollama API Key (云端服务需要)
# 本地 Ollama 不需要，留空即可
# 云端 Ollama 请填写你的 API Key
OLLAMA_API_KEY=

# 使用的模型
# 本地模型: llama3.2:latest, llama3.1, mistral, etc.
# 云端模型: gpt-oss:120b-cloud (或其他云端模型名称)
MODEL=gpt-oss:20b-cloud

# 重试配置
MAX_RETRIES=2
RETRY_DELAY=1

# Flask 环境
FLASK_ENV=development
FLASK_DEBUG=1
